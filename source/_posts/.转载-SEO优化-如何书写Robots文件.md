> 版权属于：Naiel
> 本文链接：https://www.phenxso.com/archives/150.html
> 转载时须注明出处及本声明

## 什么是robots.txt
<!-- more -->
robots是网站跟爬虫间的协议，用简单直接的txt格式文本方式告诉对应的爬虫被允许的权限，也就是说robots.txt是搜索引擎中访问网站的时候要查看的第一个文件。当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。

**简单说来它就是一个爬虫协议，用来引导爬虫如何访问你的网站。**

## robots.txt存放位置

robots.txt 只能放在网站的根目录下。
即： [http://www.domain.com/robots.txt](http://www.domain.com/robots.txt)

## 写一个robots.txt

robots文件不存在或者为空，都代表着爬虫可以随意爬取本网站所有目录。

## User\-agent:

User\-agent:(爬虫名称)
通常为同一设置则写为 `User-agent: *`
如需独立设置可以把 `*`换成对应的爬虫名称

常见搜索引擎爬虫名称 开合

### Google:

Googlebot：google网页爬虫
Googlebot\-news：google新闻爬虫
Googlebot\-image：google图片爬虫
Googlebot\-video：google视频爬虫
Googlebot\-mobile：google移动爬虫
Mediapartners\-google或Mediapartners(googlebot)：google广告爬虫
Adsbot\-google：google着陆页质量检测爬虫

### 百度：

Baiduspider：百度网页爬虫兼移动爬虫
Baiduspider\-image：百度图片爬虫
Baiduspider\-video：百度视频爬虫
Baiduspider\-news：百度新闻爬虫
Baiduspider\-favo：百度搜藏爬虫
Baiduspider\-cpro：百度联盟爬虫
Baiduspider\-ads：百度商务爬虫

### 好搜：

360spider或haosouspider：好搜网页爬虫兼移动爬虫
360spider\-image：好搜图片爬虫
360spider\-video：好搜视频爬虫

### 搜狗：

Sogou spider：搜狗综合爬虫

### 新浪爱问

Iaskspider：新浪爱问爬虫

### 有道

YodaoBot：网易有道爬虫

### Alexa

ia\_archiver：Alexa爬虫

### 雅虎

Yahoo! Slurp：雅虎爬虫

### 必应

Bingbot：必应爬虫

## Disallow:

Disallow:（路径、文件名）

表示不允许爬虫爬取匹配该路径、文件名的url。
robots.txt文件中，至少要有一条Disallow记录，否则文件失效。
但是要注意写法

```
User-agent: *
Disallow:
```

> 上面代表着允许所有爬虫爬取所有目录！

```
User-agent: *
Disallow: /
```

> 上面代表着不允许所有爬虫爬取所有目录！

一个`/`引起的灾难要注意

## Allow:

Allow:（路径、文件名）
表示允许爬取的url信息。

## Sitemap网页地图

Sitemap其实它和robots.txt一样，也是是一种收录协议，可以说是robots.txt的一种扩展。
它有自己的语话和结构。
sitemap可以使用在线生成工具生成，也可以使用插件自动完成。介绍一个在线生成工具： xml\-sitemaps

> sitemap文件一般也放在根目录，但不是必须。
> robots.txt中引入sitemap文件时，必须用绝对路径，如： [http://www.domin.com/sitemap.xml](http://www.domin.com/sitemap.xml)

## robots.txt通配符

通配符 \* 和 $ 和 #

`*`： 任意字符。例如很长的url`https://wl.phenxso.com/uploads/big/2d57148bf5dd5e7f6cc2b4e39f1dacc6.png`，禁止爬虫爬取可以这样写`Disallow: /*/big*`，这样所有url内有`/big`这样的字符都不会被爬取。
`$`： 匹配url的末端。例如不想爬虫爬取jpg图片，那么可以这样写 `Disallow: .jpg$`
`#`： 爬虫会忽略`#`后的所有字符，注释用

在一些robots.txt中可以看到有`?`这个符号，这个不是通配符。它的意义代表动态网页，例如：在robot。txt中写一条`Disallow: /*?*`。这代表着禁止爬取所有动态网页。

## 书写规范

1. 文件名`robots.txt`小写
2. `User-agent、Disallow、Allow、Sitemap`后无空格跟英文冒号，冒号后接英文空格写路径、文件名
3. `User-agent、Disallow、Allow、Sitemap`首字母大写
4. 除了第三条说明的大写其他全部小写
5. `Allow`写在`Disallow`之前
6. 每一条顶头书写

## 通配符匹配

每个爬虫的通配符匹配效果并不一样例如
下面有两个url、两种通配符写法、两种爬虫

url:
[https://www.phenxso.com/a/view.php?share/file&sid=PdmKMvz](https://www.phenxso.com/a/view.php?share/file&sid=PdmKMvz)
[https://www.phenxso.com/a/view.php?share/fileProxy&sid=PdmKMvz](https://www.phenxso.com/a/view.php?share/fileProxy&sid=PdmKMvz)

robots写法:

```
//第一种
Allow: /*/file*
Disallow: /*/*Proxy*

//第二种
Allow: /*/*share/*
Disallow: /*/*Proxy*
```

google爬虫第一种、第二种写法测试结果：
第一条url匹配，爬取成功
第二条url不匹配，爬取失败

百度爬虫第一种法测试结果：
第一条url匹配，爬取成功
第二条url不匹配，爬取失败

百度爬虫第二种法测试结果：
第一条url匹配，爬取成功
第二条url匹配，爬取成功

不同的爬虫算法不一样，在书写的时候可能需要做不同爬虫的测试。为爬虫写独立规则

不同爬虫不同规则书写方法，以下是例子：

```
User-agent: 爬虫名称
Disallow: /

User-agent: 爬虫名称
Disallow: /
```

